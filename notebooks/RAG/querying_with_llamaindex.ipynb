{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d6f8f3",
   "metadata": {},
   "source": [
    "# Querying Stage\n",
    "\n",
    "This is a working notebook to write and test the code that is used in our Google Cloud function.\n",
    "\n",
    "In this stage, the RAG pipeline extracts the most pertinent context based on a user’s query and forwards it, along with the query, to the LLM to generate a response. This procedure equips the LLM with current knowledge that wasn’t included in its original training data. This also reduces the likelihood of hallucinations, a problem for LLMs when they invent answers for data they were insufficiently trained with. The pivotal challenges in this phase revolve around the retrieval, coordination, and analysis across one or several knowledge bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b165d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Pydantic warnings since it's based in llamaindex\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad0ffa",
   "metadata": {},
   "source": [
    "## Hard-coded stuff in this cell that will be replaced in the cloud function\n",
    "* OPEN AI Key will be an environment variable\n",
    "* Weaviate IP address that we will work on finding programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa7d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import weaviate\n",
    "from weaviate import Client\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from the environment variables\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# Set the OpenAI key as an Environment Variable (for when it's run on GCS)\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY\n",
    "\n",
    "# Current Weaviate IP\n",
    "WEAVIATE_IP_ADDRESS = \"34.145.246.242\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcefacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt to exclude out of context answers\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# template = (\"We have provided context information below. If the answer to a query is not contained in this context, \"\n",
    "# #             \"please only reply that it is not in the context.\"\n",
    "# #             \"please only reply that it is not in the context. If your response will be financial in nature, \"\n",
    "#             \"make the first character of the completion 1, and if it is not financial, make the first character 0.\"\n",
    "#             \"After this initial number, 0 or 1, please continue your response as instructed previously.\"\n",
    "#             \"\\n---------------------\\n\"\n",
    "#             \"{context_str}\"\n",
    "#             \"\\n---------------------\\n\"\n",
    "#             \"Given this information, please answer the question: {query_str}\\n\"\n",
    "# )\n",
    "\n",
    "template = (\"We have provided context information below. If the answer to a query is not contained in this context, \"\n",
    "            \"please explain that the context does not include the information. If the information IS included in the context, \"\n",
    "            \"please answer the question using the context provided. However, do not refer to the context specifically by \"\n",
    "            \"saying something like 'According to the context'.\"\n",
    "            \"\\n---------------------\\n\"\n",
    "            \"{context_str}\"\n",
    "            \"\\n---------------------\\n\"\n",
    "            \"Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad22a3",
   "metadata": {},
   "source": [
    "## Hard-coded stuff in this cell that will be replaced in the cloud function\n",
    "* The websiteAddress will be from the query string of the https request\n",
    "* The timestamp will be from the query string of the https request\n",
    "* The query will be from the query string of the https request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96b3c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iankelk/anaconda3/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.24.1. The latest version is 4.4.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# client setup\n",
    "client = weaviate.Client(url=\"http://\" + WEAVIATE_IP_ADDRESS + \":8080\")\n",
    "\n",
    "# construct vector store\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"Pages\", text_key=\"text\")\n",
    "\n",
    "# setting up the indexing strategy \n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# setup an index for the Vector Store\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "\n",
    "# Create exact match filters for websiteAddress and timestamp\n",
    "website_address_filter = ExactMatchFilter(key=\"websiteAddress\", value=\"hume.ai\")\n",
    "timestamp_filter = ExactMatchFilter(key=\"timestamp\", value=\"2024-02-05T22-28-03\")\n",
    "\n",
    "# Create a metadata filters instance with the above filters\n",
    "metadata_filters = MetadataFilters(filters=[website_address_filter, timestamp_filter])\n",
    "\n",
    "# Create a query engine with the custom prompt and filters\n",
    "query_engine = index.as_query_engine(text_qa_template=qa_template,\n",
    "                                     streaming=True,\n",
    "                                     filters=metadata_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653337f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hume AI is a research lab and technology company that aims to pave the way for an ethical, human-centric future for technology that understands how we express ourselves. They provide experimentally derived datasets, models, and APIs for technology that is guided by empathy and the pursuit of human well-being. Hume AI's solutions are based on cutting-edge research published in top scientific journals."
     ]
    }
   ],
   "source": [
    "# Execute the query\n",
    "query_str = \"What is Hume?\"\n",
    "streaming_response = query_engine.query(query_str)\n",
    "\n",
    "def process_streaming_response(streaming_response):\n",
    "    try:\n",
    "        for text in streaming_response.response_gen:\n",
    "            if text:   # Check for null character or empty string\n",
    "                print(text, end=\"\", flush=True)\n",
    "    except asyncio.CancelledError as e:\n",
    "        print('Streaming cancelled', flush=True)\n",
    "\n",
    "process_streaming_response(streaming_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988dc219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following websites were used as references:\n",
      "\n",
      "['https://dev.hume.ai/docs/hume', 'https://dev.hume.ai/docs/introduction']\n"
     ]
    }
   ],
   "source": [
    "def extract_document_urls(streaming_response):\n",
    "    urls = []\n",
    "    for node_with_score in streaming_response.source_nodes:\n",
    "        relationships = node_with_score.node.relationships\n",
    "        for related_node_info in relationships.values():\n",
    "            if related_node_info.node_type == \"4\":  # Corresponds to ObjectType.DOCUMENT\n",
    "                urls.append(related_node_info.node_id)\n",
    "    return urls\n",
    "\n",
    "extracted_urls = extract_document_urls(streaming_response)\n",
    "print(\"The following websites were used as references:\\n\")\n",
    "print(extracted_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309f7081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not provide any information about Kim Kardashian."
     ]
    }
   ],
   "source": [
    "query_str = \"Who is Kim Kardashian?\"\n",
    "streaming_response = query_engine.query(query_str)\n",
    "\n",
    "# Print the response as it arrives\n",
    "# streaming_response.print_response_stream()\n",
    "process_streaming_response(streaming_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d3503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following websites were used as references:\n",
      "\n",
      "['https://dev.hume.ai/docs/too-many-face-identifiers', 'https://dev.hume.ai/docs/the-platform']\n"
     ]
    }
   ],
   "source": [
    "extracted_urls = extract_document_urls(streaming_response)\n",
    "print(\"The following websites were used as references:\\n\")\n",
    "print(extracted_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
